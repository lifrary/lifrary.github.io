
<!DOCTYPE html>
<html>

<head lang="en">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48YJ5MFMGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48YJ5MFMGM');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AADiff</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://reyllama.github.io/OneR"/>
    <meta property="og:title" content="Fashion-Diffusion" />
    <meta property="og:description" content="Project page for Unifying Vision-Language Representation Space with Single-tower Transformer" />




<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion<br>
                <small>
                    CVPR 2023 Workshop on AI for Content Creation
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://www.linkedin.com/in/seungwoo-dale-lee/">
                          Seungwoo Lee
                        </a>
                        </br>Seoul National University
                    </li>
                    <li>
                        <a href="https://reyllama.github.io">
                          Chaerin Kong
                        </a>
                        </br>Seoul National University
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/jdh3577/">
                          Donghyeon Jeon
                        </a>
                        </br>NAVER
                    </li>
                    <li>
                        <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">
                          Nojun Kwak
                        </a>
                        </br>Seoul National University
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2305.04001">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://github.com/reyllama/caption-diffusion">-->
<!--                            <image src="img/github.png" height="60px">-->
<!--                                <h4><strong>Code</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/method14.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Recent advances in diffusion models have showcased promising results in the text-to-video (T2V) synthesis task.
                    However, as these T2V models solely employ text as the guidance, they tend to struggle in modeling detailed temporal dynamics.
                    In this paper, we introduce a novel T2V framework that additionally employ audio signals to control the temporal dynamics,
                    empowering an off-the-shelf T2I diffusion to generate audio-aligned videos. We propose audio-based regional editing and
                    signal smoothing to strike a good balance between the two contradicting desiderata of video synthesis, i.e., temporal flexibility and coherence.
                    We empirically demonstrate the effectiveness of our method through experiments, and further present practical applications for contents creation.
                </p>
            </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Multimodal Objectives as Generative Guidance-->
<!--                </h3>-->
<!--                <p>-->
<!--                    Our goal is to study the properties of multimodal objectives by visualizing their guidance with a diffusion model. To that end, we employ two pretrained models.-->
<!--                    For the generative backbone, we use a generic unconditional diffusion model trained on Imagenet (Dhariwal & Nichol, 2021). This model suits our purpose because-->
<!--                    1) it is unconditional, which means the capacity for class-conditioned synthesis solely depends on the classifier guidance and 2) it has just the right level of-->
<!--                    generative capacity to faithfully visualize semantic signals but not to step further and compensate for their blind spots. For the guidance model, we use BLIP (Li et al., 2022a),-->
<!--                    which is pretrained on 129M image-text pairs and supports image-text contrastive (<em>ITC</em>), image-text matching (<em>ITM</em>) and captioning (<cap>CAP</cap>).-->
<!--                    This minimizes unwanted compounding effects from using multiple models with differing specs. We use the classifier-guided diffusion introduced in Dhariwal & Nichol (2021) with-->
<!--                    compute-efficient modifications of <em>Avrahami et al. (2022)</em>.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Findings-->
<!--                </h3>-->
<!--                <image src="img/newfig1.jpg" class="img-responsive" alt="overview">-->
<!--                <br>-->
<!--                <p>-->
<!--                    <em>1. While ITC focuses on the fine details of the salient object, CAP tends to reason about the global scene composition.</em>-->
<!--                </p>-->
<!--                <p>-->
<!--                    <em>2. ITC commonly lumps visual semantics together to forcefully form a global semantic.</em>-->
<!--                </p>-->
<!--                <p>-->
<!--                    <em>3. Patch-token cross-attention plays a key role in fine-grained visual understanding.</em>-->
<!--                </p>-->
<!--                <p>-->
<!--                    <em>4. Dense supervision makes the representations more robust to noise perturbations.</em>-->
<!--                </p>-->
<!--                <br>-->
<!--                <image src="img/fig5.jpg" class="img-responsive" alt="overview">-->
<!--                <br>-->
<!--                <p>-->
<!--                    <em>5. CAP is a more indirect if not challenging form of supervision than ITC or ITM.</em>-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    New Baseline: Guidance Shift-->
<!--                </h3>-->
<!--                <p>-->
<!--                    Based on the above findings, we propose a simple yet effective modification to the previous regime that takes advantage of both ends.-->
<!--                    Specifically, we start from <em>CAP</em> guidance to outline the overall scene structure and gradually shift towards <em>ITC</em> for refined details.-->
<!--                    We compare this with naive baselines, simple <em>ITC</em>, simple <em>CAP</em> and <em>BLEND</em>, where we use these two signals but simply mix them without the gradual transition.-->
<!--                    We report both qualitative and quantitative human evaluations. Ours not only supports our empirical insights but also improves complex scene generation in an extremely straightforward manner.-->
<!--                </p>-->
<!--                <image src="img/fig7.jpg" class="img-responsive" alt="overview">-->
<!--                    <br>-->
<!--                <image src="img/fig6.jpg" class="img-responsive" alt="overview">-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Additional Results-->
<!--                </h3>-->
<!--                <p>-->
<!--                    We hereby display additional examples that further clarifies our analysis.-->
<!--                </p>-->
<!--                <image src="img/sup1.jpg" class="img-responsive" alt="overview">-->
<!--                    <br>-->
<!--                <image src="img/sup2.jpg" class="img-responsive" alt="overview">-->
<!--                <br>-->
<!--                <p>As opposed to <em>ITC</em> that generates realistic samples but the output severely oscillate even with minor typos, dense supervisions shows much better robustness, which can come in handy in a typical V-L setting-->
<!--                where we rely on a massive web-crawled noisy image-text database.</p>-->
<!--                <br>-->
<!--                <image src="img/sup4.jpg" class="img-responsive" alt="overview">-->
<!--                    <br>-->
<!--                <image src="img/sup3.jpg" class="img-responsive" alt="overview">-->
<!--                    <br><br>-->
<!--            </div>-->
<!--        </div>-->


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Citation-->
<!--                </h3>-->
<!--                <div class="form-group col-md-10 col-md-offset-1">-->
<!--                    <textarea id="bibtex" class="form-control" readonly>-->
<!--@article{barron2021mipnerf,-->
<!--    title={Mip-NeRF: A Multiscale Representation -->
<!--           for Anti-Aliasing Neural Radiance Fields},-->
<!--    author={Jonathan T. Barron and Ben Mildenhall and -->
<!--            Matthew Tancik and Peter Hedman and -->
<!--            Ricardo Martin-Brualla and Pratul P. Srinivasan},-->
<!--    journal={ICCV},-->
<!--    year={2021}-->
<!--}</textarea>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Acknowledgements-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                We thank Janne Kontkanen and David Salesin for their comments on the text, Paul Debevec for constructive discussions, and Boyang Deng for JaxNeRF. -->
<!--                    <br>-->
<!--                MT is funded by an NSF Graduate Fellowship.-->
<!--                    <br>-->
<!--                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
    </div>
</body>
</html>
